{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Medical Q&A Assistant using Qwen3-4B\n",
        "\n",
        "## Setup and Environment Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa2cdfd8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: Install required packages\n",
        "!pip install -q transformers torch numpy pandas matplotlib tqdm colorama accelerate bitsandbytes sentencepiece einops ollama requests scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79473dba",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: Import necessary libraries\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import re\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.auto import tqdm\n",
        "from colorama import Fore, Style\n",
        "import logging\n",
        "import requests\n",
        "import gc\n",
        "from datetime import datetime\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Check GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "logger.info(f\"Using device: {device}\")\n",
        "\n",
        "# Memory management function\n",
        "def clear_memory():\n",
        "    \"\"\"Clear GPU memory and garbage collect\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    logger.info(\"Memory cleared\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd641387",
      "metadata": {},
      "source": [
        "## Model Loading and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d71a2250",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: Load Qwen3-4B Model\n",
        "def load_model(model_name=\"Qwen/Qwen1.5-4B-Chat\", use_4bit=True):\n",
        "    \"\"\"\n",
        "    Load the Qwen3-4B model with optimized settings for virtual environments\n",
        "    \"\"\"\n",
        "    logger.info(f\"Loading model: {model_name}\")\n",
        "    \n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_name, \n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.padding_side = \"left\"\n",
        "    \n",
        "    # Load model with optimizations for limited resources\n",
        "    model_kwargs = {\n",
        "        \"device_map\": \"auto\",\n",
        "        \"trust_remote_code\": True,\n",
        "    }\n",
        "    \n",
        "    # Use 4-bit quantization for memory efficiency if requested\n",
        "    if use_4bit:\n",
        "        model_kwargs.update({\n",
        "            \"load_in_4bit\": True,\n",
        "            \"bnb_4bit_compute_dtype\": torch.bfloat16,\n",
        "            \"bnb_4bit_quant_type\": \"nf4\",\n",
        "        })\n",
        "    else:\n",
        "        model_kwargs[\"torch_dtype\"] = torch.bfloat16\n",
        "    \n",
        "    try:\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            **model_kwargs\n",
        "        )\n",
        "        \n",
        "        # Create text generation pipeline\n",
        "        pipe = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            max_new_tokens=512,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.1\n",
        "        )\n",
        "        \n",
        "        logger.info(\"Model loaded successfully!\")\n",
        "        return tokenizer, model, pipe\n",
        "    \n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading model: {str(e)}\")\n",
        "        raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b5749e4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Load model with memory monitoring\n",
        "import psutil\n",
        "\n",
        "def log_memory_usage():\n",
        "    \"\"\"Log current memory usage\"\"\"\n",
        "    process = psutil.Process(os.getpid())\n",
        "    mem_info = process.memory_info()\n",
        "    logger.info(f\"Memory usage: {mem_info.rss / 1024**2:.2f} MB\")\n",
        "\n",
        "# Log initial memory usage\n",
        "log_memory_usage()\n",
        "\n",
        "# Load model\n",
        "tokenizer, model, pipe = load_model(use_4bit=True)\n",
        "\n",
        "# Log memory usage after model loading\n",
        "log_memory_usage()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "058d44ae",
      "metadata": {},
      "source": [
        "## Prompt Templates and Strategies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "057f5b04",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: Load prompt templates\n",
        "def load_prompt_template(file_path):\n",
        "    \"\"\"Load prompt template from file\"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r') as f:\n",
        "            return f.read()\n",
        "    except FileNotFoundError:\n",
        "        logger.error(f\"Prompt template file not found: {file_path}\")\n",
        "        # Provide fallback templates if files don't exist\n",
        "        if \"zero_shot\" in file_path:\n",
        "            return \"\"\"As a medical information assistant, please answer the following healthcare question with accurate information. If you're uncertain, clearly state the limitations of your knowledge.\n",
        "\n",
        "IMPORTANT MEDICAL DISCLAIMER: This information is for educational purposes only and is not a substitute for professional medical advice. Always consult with a qualified healthcare provider for medical concerns.\n",
        "\n",
        "Question: {{QUERY}}\"\"\"\n",
        "        elif \"few_shot\" in file_path:\n",
        "            return \"\"\"As a medical information assistant, here are some examples of how to answer healthcare questions:\n",
        "\n",
        "Question: What are the symptoms of strep throat?\n",
        "Answer: Strep throat typically presents with sudden sore throat, pain when swallowing, fever over 101Â°F, red and swollen tonsils (sometimes with white patches), tiny red spots on the roof of the mouth, and swollen lymph nodes in the neck. Unlike viral infections, strep throat usually doesn't cause cough, runny nose, or hoarseness. \n",
        "\n",
        "IMPORTANT MEDICAL DISCLAIMER: This information is for educational purposes only. A healthcare professional should confirm diagnosis through testing, as many conditions can cause similar symptoms.\n",
        "\n",
        "Question: How does insulin work in the body?\n",
        "Answer: Insulin is a hormone produced by beta cells in the pancreas that regulates blood glucose levels. When you consume carbohydrates, they break down into glucose in the bloodstream. The pancreas then releases insulin, which acts like a key that allows glucose to enter cells for energy use. In people with diabetes, either insufficient insulin is produced (Type 1) or cells become resistant to insulin's effects (Type 2), resulting in elevated blood glucose levels.\n",
        "\n",
        "IMPORTANT MEDICAL DISCLAIMER: This information is for educational purposes only and not a substitute for professional medical advice. If you have concerns about diabetes or insulin function, please consult with a healthcare provider.\n",
        "\n",
        "Question: What is the difference between Alzheimer's and dementia?\n",
        "Answer: Dementia is a general term for symptoms affecting memory, thinking, and social abilities severely enough to interfere with daily functioning. Alzheimer's disease is the most common specific cause of dementia, accounting for 60-80% of cases. While Alzheimer's is characterized by specific brain changes (amyloid plaques and tau tangles), dementia can be caused by various conditions including vascular issues, Lewy body disease, and others. All Alzheimer's patients have dementia, but not all dementia patients have Alzheimer's.\n",
        "\n",
        "IMPORTANT MEDICAL DISCLAIMER: This information is for educational purposes only. If you're concerned about cognitive changes in yourself or a loved one, please consult with a healthcare professional for proper evaluation and support.\n",
        "\n",
        "Now please answer this medical question:\n",
        "{{QUERY}}\"\"\"\n",
        "        elif \"cot\" in file_path:\n",
        "            return \"\"\"As a medical information assistant, please answer the following healthcare question. Think through your reasoning step by step before providing your final answer.\n",
        "\n",
        "Question: {{QUERY}}\n",
        "\n",
        "Let me reason through this systematically:\n",
        "1) First, I'll consider what medical concepts are involved in this question\n",
        "2) Then, I'll analyze relevant mechanisms, causes, or processes\n",
        "3) Next, I'll evaluate important distinctions or differential considerations\n",
        "4) Finally, I'll formulate a clear, accurate answer based on established medical knowledge\n",
        "\n",
        "Reasoning:\"\"\"\n",
        "        else:  # meta prompt\n",
        "            return \"\"\"As a medical information assistant, I'll answer the following healthcare question by breaking it down into sub-questions I need to answer first.\n",
        "\n",
        "Main Question: {{QUERY}}\n",
        "\n",
        "Let me ask myself some clarifying questions:\n",
        "1) What specific medical concepts do I need to understand to answer this question?\n",
        "2) What are the key mechanisms or processes involved?\n",
        "3) Are there any important distinctions or considerations I should address?\n",
        "4) What limitations exist in my knowledge about this topic?\n",
        "5) What safety warnings or medical disclaimers should I include?\n",
        "\n",
        "Now I'll answer each of these sub-questions to build my complete response:\"\"\"\n",
        "\n",
        "# Load prompt templates\n",
        "prompt_templates = {\n",
        "    \"zero_shot\": load_prompt_template(\"prompts/zero_shot.txt\"),\n",
        "    \"few_shot\": load_prompt_template(\"prompts/few_shot.txt\"),\n",
        "    \"cot\": load_prompt_template(\"prompts/cot_prompt.txt\"),\n",
        "    \"meta\": load_prompt_template(\"prompts/meta_prompt.txt\")\n",
        "}\n",
        "\n",
        "# Define prompt functions\n",
        "def zero_shot_prompt(query):\n",
        "    return prompt_templates[\"zero_shot\"].replace(\"{{QUERY}}\", query)\n",
        "\n",
        "def few_shot_prompt(query):\n",
        "    return prompt_templates[\"few_shot\"].replace(\"{{QUERY}}\", query)\n",
        "\n",
        "def cot_prompt(query):\n",
        "    return prompt_templates[\"cot\"].replace(\"{{QUERY}}\", query)\n",
        "\n",
        "def meta_prompt(query):\n",
        "    return prompt_templates[\"meta\"].replace(\"{{QUERY}}\", query)\n",
        "\n",
        "# Define prompt types dictionary\n",
        "prompt_types = {\n",
        "    \"zero_shot\": zero_shot_prompt,\n",
        "    \"few_shot\": few_shot_prompt,\n",
        "    \"cot\": cot_prompt,\n",
        "    \"meta\": meta_prompt\n",
        "}\n",
        "\n",
        "logger.info(\"Prompt templates loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1941ea67",
      "metadata": {},
      "source": [
        "## Medical Safety and Disclaimer Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cde71546",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: Medical safety functions\n",
        "def add_medical_disclaimer(response):\n",
        "    \"\"\"Add medical disclaimer if not already present\"\"\"\n",
        "    disclaimer = \"\\n\\nIMPORTANT MEDICAL DISCLAIMER: This information is for educational purposes only and is not a substitute for professional medical advice. Always consult with a qualified healthcare provider for medical concerns.\"\n",
        "    \n",
        "    if \"DISCLAIMER\" not in response and \"disclaimer\" not in response.lower():\n",
        "        return response + disclaimer\n",
        "    return response\n",
        "\n",
        "def check_emergency_symptoms(query, response):\n",
        "    \"\"\"Check for emergency symptoms that require immediate attention\"\"\"\n",
        "    emergency_symptoms = [\n",
        "        \"chest pain\", \"severe bleeding\", \"difficulty breathing\", \"shortness of breath\",\n",
        "        \"sudden numbness\", \"sudden weakness\", \"sudden confusion\", \"sudden severe headache\",\n",
        "        \"sudden vision loss\", \"suicidal\", \"suicide\", \"heart attack\", \"stroke\"\n",
        "    ]\n",
        "    \n",
        "    emergency_warning = \"\\n\\nâ ï¸ EMERGENCY WARNING: The symptoms described may indicate a serious medical condition requiring immediate attention. Please seek emergency medical care immediately by calling emergency services or going to the nearest emergency room.\"\n",
        "    \n",
        "    # Check if any emergency symptoms are mentioned in the query\n",
        "    if any(symptom in query.lower() for symptom in emergency_symptoms):\n",
        "        if \"EMERGENCY WARNING\" not in response and \"emergency\" not in response.lower():\n",
        "            return response + emergency_warning\n",
        "    \n",
        "    return response\n",
        "\n",
        "def ensure_response_safety(query, response):\n",
        "    \"\"\"Ensure response includes proper medical safety elements\"\"\"\n",
        "    # Add medical disclaimer\n",
        "    response = add_medical_disclaimer(response)\n",
        "    \n",
        "    # Check for emergency symptoms\n",
        "    response = check_emergency_symptoms(query, response)\n",
        "    \n",
        "    # Avoid diagnostic language\n",
        "    response = avoid_diagnostic_language(response)\n",
        "    \n",
        "    return response\n",
        "\n",
        "def avoid_diagnostic_language(response):\n",
        "    \"\"\"Replace diagnostic language with more appropriate phrasing\"\"\"\n",
        "    diagnostic_patterns = [\n",
        "        (r\"you have ([a-zA-Z\\s]+)\", r\"you may be experiencing symptoms consistent with \\1\"),\n",
        "        (r\"you are suffering from ([a-zA-Z\\s]+)\", r\"you may be experiencing \\1\"),\n",
        "        (r\"you definitely have ([a-zA-Z\\s]+)\", r\"your symptoms may be consistent with \\1\"),\n",
        "        (r\"you should take ([a-zA-Z0-9\\s]+)\", r\"some healthcare providers may recommend \\1, but consult your doctor\"),\n",
        "        (r\"I diagnose you with ([a-zA-Z\\s]+)\", r\"these symptoms are sometimes associated with \\1\")\n",
        "    ]\n",
        "    \n",
        "    for pattern, replacement in diagnostic_patterns:\n",
        "        response = re.sub(pattern, replacement, response, flags=re.IGNORECASE)\n",
        "    \n",
        "    return response\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c90682e2",
      "metadata": {},
      "source": [
        "## Query Processing and Response Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23d97ea2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7: Generate medical responses\n",
        "def generate_medical_response(query, prompt_type=\"cot\"):\n",
        "    \"\"\"Generate a medical response using the specified prompt type\"\"\"\n",
        "    try:\n",
        "        # Get the appropriate prompt function\n",
        "        prompt_func = prompt_types.get(prompt_type, cot_prompt)\n",
        "        \n",
        "        # Format the prompt\n",
        "        prompt = prompt_func(query)\n",
        "        \n",
        "        # Generate response\n",
        "        logger.info(f\"Generating response using {prompt_type} prompt\")\n",
        "        result = pipe(prompt, return_full_text=False)[0][\"generated_text\"]\n",
        "        \n",
        "        # Ensure response safety\n",
        "        safe_response = ensure_response_safety(query, result)\n",
        "        \n",
        "        return {\n",
        "            \"query\": query,\n",
        "            \"prompt_type\": prompt_type,\n",
        "            \"prompt\": prompt,\n",
        "            \"raw_response\": result,\n",
        "            \"safe_response\": safe_response,\n",
        "            \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        }\n",
        "    \n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error generating response: {str(e)}\")\n",
        "        return {\n",
        "            \"query\": query,\n",
        "            \"prompt_type\": prompt_type,\n",
        "            \"error\": str(e),\n",
        "            \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66f22a1d",
      "metadata": {},
      "source": [
        "## Ambiguity Handling and Clarification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bac23d27",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 8: Ambiguity handling\n",
        "def handle_ambiguous_input(query, model_response):\n",
        "    \"\"\"\n",
        "    Detect ambiguous responses and generate clarification prompts\n",
        "    \"\"\"\n",
        "    # Check if response indicates uncertainty or ambiguity\n",
        "    uncertainty_indicators = [\n",
        "        \"unclear\", \"ambiguous\", \"could mean\", \"need more information\",\n",
        "        \"I'm not sure\", \"it depends\", \"could refer to\", \"insufficient details\"\n",
        "    ]\n",
        "    \n",
        "    # Check if the response is too short (might indicate confusion)\n",
        "    is_short_response = len(model_response.split()) < 20\n",
        "    \n",
        "    # Check if response contains multiple conflicting possibilities\n",
        "    has_multiple_possibilities = \"on one hand\" in model_response.lower() and \"on the other hand\" in model_response.lower()\n",
        "    \n",
        "    if (any(indicator in model_response.lower() for indicator in uncertainty_indicators) or \n",
        "        is_short_response or has_multiple_possibilities):\n",
        "        \n",
        "        # Generate clarification prompt based on the query type\n",
        "        if \"symptoms\" in query.lower() or \"signs\" in query.lower():\n",
        "            clarification = f\"\"\"I notice your question about \"{query}\" could benefit from more details:\n",
        "            \n",
        "1) Could you specify how long you've been experiencing these symptoms?\n",
        "2) Are there any other symptoms you're experiencing alongside these?\n",
        "3) Are you asking about general information or concerned about specific symptoms you're experiencing?\n",
        "\n",
        "IMPORTANT MEDICAL DISCLAIMER: I can provide general medical information but cannot diagnose conditions or replace professional medical advice. If you're experiencing concerning symptoms, please consult a healthcare provider.\"\"\"\n",
        "        \n",
        "        elif \"medication\" in query.lower() or \"drug\" in query.lower() or \"medicine\" in query.lower():\n",
        "            clarification = f\"\"\"I notice your question about \"{query}\" could be made more specific:\n",
        "            \n",
        "1) Are you asking about specific dosages, side effects, or interactions?\n",
        "2) Do you have any other medical conditions or take other medications that might be relevant?\n",
        "3) Are you looking for general information or have concerns about a specific situation?\n",
        "\n",
        "IMPORTANT MEDICAL DISCLAIMER: I can provide general medication information but cannot give personalized medical advice. Always consult a healthcare provider or pharmacist for guidance on medications.\"\"\"\n",
        "        \n",
        "        else:\n",
        "            clarification = f\"\"\"I notice your question about \"{query}\" could be interpreted in multiple ways:\n",
        "            \n",
        "1) Could you provide more context or specific details about your question?\n",
        "2) Are you looking for general information or information about a specific situation?\n",
        "3) Would it help if I explained some of the common terms or concepts related to this topic first?\n",
        "\n",
        "IMPORTANT MEDICAL DISCLAIMER: I can provide general medical information but cannot diagnose conditions or replace professional medical advice. For medical concerns, please consult a qualified healthcare provider.\"\"\"\n",
        "        \n",
        "        return clarification\n",
        "    \n",
        "    return model_response\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b66f9d39",
      "metadata": {},
      "source": [
        "## Hallucination Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2a74847",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 9: Hallucination detection\n",
        "def detect_hallucinations(response, query, expected_concepts=None):\n",
        "    \"\"\"\n",
        "    Detect potential hallucinations in model responses\n",
        "    Returns a hallucination score and flagged issues\n",
        "    \"\"\"\n",
        "    hallucination_indicators = {\n",
        "        \"definitive_claims\": [\n",
        "            r\"100% effective\", r\"always works\", r\"cures all\", r\"completely safe\",\n",
        "            r\"guaranteed to\", r\"never causes\", r\"all patients\", r\"everyone with\"\n",
        "        ],\n",
        "        \"specific_numbers\": [\n",
        "            r\"\\d{2,3}% of (patients|people|cases)\", r\"studies show \\d{2,3}%\"\n",
        "        ],\n",
        "        \"unverifiable_claims\": [\n",
        "            r\"recent studies show\", r\"doctors agree that\", r\"research has proven\",\n",
        "            r\"it is well established\", r\"it is widely accepted\"\n",
        "        ],\n",
        "        \"medical_advice\": [\n",
        "            r\"you should\", r\"you must\", r\"you need to\", r\"I recommend\",\n",
        "            r\"take \\d+ (mg|milligrams)\", r\"increase your dosage\", r\"reduce your dosage\"\n",
        "        ]\n",
        "    }\n",
        "    \n",
        "    issues = []\n",
        "    hallucination_score = 0\n",
        "    \n",
        "    # Check for definitive claims\n",
        "    for pattern in hallucination_indicators[\"definitive_claims\"]:\n",
        "        if re.search(pattern, response, re.IGNORECASE):\n",
        "            issues.append(f\"Definitive claim detected: '{re.search(pattern, response, re.IGNORECASE).group(0)}'\")\n",
        "            hallucination_score += 2\n",
        "    \n",
        "    # Check for specific unverifiable numbers\n",
        "    for pattern in hallucination_indicators[\"specific_numbers\"]:\n",
        "        if re.search(pattern, response, re.IGNORECASE):\n",
        "            issues.append(f\"Specific unverifiable statistic: '{re.search(pattern, response, re.IGNORECASE).group(0)}'\")\n",
        "            hallucination_score += 1.5\n",
        "    \n",
        "    # Check for unverifiable claims\n",
        "    for pattern in hallucination_indicators[\"unverifiable_claims\"]:\n",
        "        if re.search(pattern, response, re.IGNORECASE):\n",
        "            issues.append(f\"Unverifiable claim: '{re.search(pattern, response, re.IGNORECASE).group(0)}'\")\n",
        "            hallucination_score += 1\n",
        "    \n",
        "    # Check for medical advice\n",
        "    for pattern in hallucination_indicators[\"medical_advice\"]:\n",
        "        if re.search(pattern, response, re.IGNORECASE):\n",
        "            issues.append(f\"Medical advice detected: '{re.search(pattern, response, re.IGNORECASE).group(0)}'\")\n",
        "            hallucination_score += 2\n",
        "    \n",
        "    # Check for expected concepts (if provided)\n",
        "    if expected_concepts:\n",
        "        found_concepts = 0\n",
        "        for concept in expected_concepts:\n",
        "            if concept.lower() in response.lower():\n",
        "                found_concepts += 1\n",
        "        \n",
        "        concept_coverage = found_concepts / len(expected_concepts)\n",
        "        if concept_coverage < 0.5:\n",
        "            issues.append(f\"Low coverage of expected medical concepts: {found_concepts}/{len(expected_concepts)}\")\n",
        "            hallucination_score += 1\n",
        "    \n",
        "    # Normalize score between 0-5 (for evaluation criteria)\n",
        "    normalized_score = min(5, hallucination_score / 2)\n",
        "    \n",
        "    return {\n",
        "        \"score\": normalized_score,\n",
        "        \"issues\": issues,\n",
        "        \"severity\": \"High\" if normalized_score > 3.5 else \"Medium\" if normalized_score > 2 else \"Low\"\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1957f6a2",
      "metadata": {},
      "source": [
        "## Interactive Demo and Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "010d64f3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 10: Test with sample queries\n",
        "def format_response(response_data):\n",
        "    \"\"\"Format a response for display\"\"\"\n",
        "    output = f\"Query: {response_data['query']}\\n\\n\"\n",
        "    output += f\"Prompt Type: {response_data['prompt_type']}\\n\\n\"\n",
        "    output += f\"Response:\\n{response_data['safe_response']}\\n\\n\"\n",
        "    \n",
        "    # Add hallucination check\n",
        "    hallucination_check = detect_hallucinations(response_data['safe_response'], response_data['query'])\n",
        "    if hallucination_check['score'] > 0:\n",
        "        output += f\"â ï¸ Hallucination Score: {hallucination_check['score']}/5 ({hallucination_check['severity']})\\n\"\n",
        "        if hallucination_check['issues']:\n",
        "            output += \"Issues detected:\\n\"\n",
        "            for issue in hallucination_check['issues']:\n",
        "                output += f\"- {issue}\\n\"\n",
        "    else:\n",
        "        output += \"â No hallucinations detected\\n\"\n",
        "    \n",
        "    return output\n",
        "\n",
        "# Sample queries from the project specification\n",
        "sample_queries = [\n",
        "    \"What are the early symptoms of diabetes?\",\n",
        "    \"Explain what hypertension means in simple terms\",\n",
        "    \"Should I be worried about chest pain?\",\n",
        "    \"What causes frequent headaches?\",\n",
        "    \"When should someone see a cardiologist?\"\n",
        "]\n",
        "\n",
        "# Test each query with each prompt type\n",
        "for query in sample_queries:\n",
        "    print(f\"\\n{'='*80}\\nTesting query: {query}\\n{'='*80}\")\n",
        "    \n",
        "    for prompt_type in prompt_types.keys():\n",
        "        response_data = generate_medical_response(query, prompt_type)\n",
        "        print(f\"\\n{'-'*40}\\nPrompt Type: {prompt_type}\\n{'-'*40}\")\n",
        "        print(format_response(response_data))\n",
        "        \n",
        "        # Clear memory between queries\n",
        "        clear_memory()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b17d6fc9",
      "metadata": {},
      "source": [
        "## Interactive Demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6b6e5df",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 11: Interactive demo\n",
        "def run_interactive_demo():\n",
        "    \"\"\"Run an interactive demo of the medical QA system\"\"\"\n",
        "    print(f\"{Fore.GREEN}===== Medical Q&A Assistant Demo =====\\n{Style.RESET_ALL}\")\n",
        "    print(f\"Using Qwen3-4B with optimal prompt strategies\")\n",
        "    print(f\"Type 'exit' to quit the demo\\n\")\n",
        "    \n",
        "    while True:\n",
        "        query = input(f\"{Fore.BLUE}Enter your medical question: {Style.RESET_ALL}\")\n",
        "        if query.lower() == 'exit':\n",
        "            break\n",
        "        \n",
        "        # Determine best prompt type for this query\n",
        "        prompt_type = \"cot\"  # Default to Chain-of-Thought\n",
        "        \n",
        "        print(f\"\\n{Fore.YELLOW}Processing with {prompt_type} prompt...{Style.RESET_ALL}\")\n",
        "        \n",
        "        # Generate response\n",
        "        response_data = generate_medical_response(query, prompt_type)\n",
        "        \n",
        "        # Check for ambiguity\n",
        "        clarification = handle_ambiguous_input(query, response_data['raw_response'])\n",
        "        if clarification != response_data['raw_response']:\n",
        "            print(f\"\\n{Fore.RED}Ambiguity detected. Requesting clarification:{Style.RESET_ALL}\")\n",
        "            print(f\"{clarification}\\n\")\n",
        "            continue\n",
        "        \n",
        "        # Format and display response\n",
        "        print(f\"\\n{format_response(response_data)}\\n\")\n",
        "        \n",
        "        # Clear memory\n",
        "        clear_memory()\n",
        "\n",
        "# Run the interactive demo\n",
        "if __name__ == \"__main__\":\n",
        "    run_interactive_demo()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7daeb0a7",
      "metadata": {},
      "source": [
        "python\n",
        "# Cell 12: Save results and clean up\n",
        "def save_results(results, filename=\"medical_qa_results.json\"):\n",
        "    \"\"\"Save results to a JSON file\"\"\"\n",
        "    with open(filename, \"w\") as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "    logger.info(f\"Results saved to {filename}\")\n",
        "\n",
        "# Clean up and free memory\n",
        "def cleanup():\n",
        "    \"\"\"Clean up resources and free memory\"\"\"\n",
        "    global model, tokenizer, pipe\n",
        "    \n",
        "    # Delete model and pipeline\n",
        "    del pipe\n",
        "    del model\n",
        "    del tokenizer\n",
        "    \n",
        "    # Clear memory\n",
        "    clear_memory()\n",
        "    \n",
        "    logger.info(\"Cleanup completed\")\n",
        "\n",
        "# Example of saving results\n",
        "sample_results = []\n",
        "for query in sample_queries[:2]:  # Just test with 2 queries to save resources\n",
        "    for prompt_type in prompt_types.keys():\n",
        "        response_data = generate_medical_response(query, prompt_type)\n",
        "        sample_results.append(response_data)\n",
        "\n",
        "# Save sample results\n",
        "save_results(sample_results, \"evaluation/sample_results.json\")\n",
        "\n",
        "# Clean up\n",
        "cleanup()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
