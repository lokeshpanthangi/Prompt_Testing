# Medical QA System with Qwen3 4B
## Implementation and Evaluation of Prompt Engineering Techniques

# [CELL 1] - Setup and Requirements Installation
```python
# Install required packages
!pip install transformers torch numpy pandas matplotlib seaborn tqdm colorama
!pip install accelerate bitsandbytes sentencepiece
!pip install -q -U einops
```

# [CELL 2] - Import Libraries
```python
import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import json
import re
import os
from datetime import datetime
from tqdm.auto import tqdm
from colorama import Fore, Style
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
```

# [CELL 3] - Load Qwen3 4B Model
```python
# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Load model and tokenizer
model_name = "Qwen/Qwen1.5-4B"  # Using Qwen1.5-4B as it's compatible with HuggingFace

print("Loading tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left"

print("Loading model...")
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    torch_dtype=torch.bfloat16,
    trust_remote_code=True
)

# Create text generation pipeline
pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7,
    top_p=0.9,
    repetition_penalty=1.1
)

print("Model loaded successfully!")
```

# [CELL 4] - Define Prompt Templates
```python
# Define the different prompt templates
def load_prompt_template(file_path):
    """Load prompt template from file"""
    with open(file_path, 'r') as f:
        return f.read()

def zero_shot_prompt(query):
    template = load_prompt_template("prompts/zero_shot.txt")
    return template.replace("{{QUERY}}", query)

def few_shot_prompt(query):
    template = load_prompt_template("prompts/few_shot.txt")
    return template.replace("{{QUERY}}", query)

def cot_prompt(query):
    template = load_prompt_template("prompts/cot_prompt.txt")
    return template.replace("{{QUERY}}", query)

def meta_prompt(query):
    template = load_prompt_template("prompts/meta_prompt.txt")
    return template.replace("{{QUERY}}", query)

# Define prompt types dictionary
prompt_types = {
    "zero_shot": zero_shot_prompt,
    "few_shot": few_shot_prompt,
    "cot": cot_prompt,
    "meta": meta_prompt
}

print("Prompt templates loaded successfully!")
```

# [CELL 5] - Create Fallback Mechanism
```python
def handle_ambiguous_input(query, model_response):
    """
    Detect ambiguous responses and generate clarification prompts
    """
    # Check if response indicates uncertainty or ambiguity
    uncertainty_indicators = [
        "unclear", "ambiguous", "could mean", "need more information",
        "I'm not sure", "it depends", "could refer to", "insufficient details"
    ]
    
    # Check if the response is too short (might indicate confusion)
    is_short_response = len(model_response.split()) < 20
    
    # Check if response contains multiple conflicting possibilities
    has_multiple_possibilities = "on one hand" in model_response.lower() and "on the other hand" in model_response.lower()
    
    if (any(indicator in model_response.lower() for indicator in uncertainty_indicators) or 
        is_short_response or has_multiple_possibilities):
        
        # Generate clarification prompt based on the query type
        if "symptoms" in query.lower() or "signs" in query.lower():
            clarification = f"""I notice your question about "{query}" could benefit from more details:
            
1) Could you specify how long you've been experiencing these symptoms?
2) Are there any other symptoms you're experiencing alongside these?
3) Are you asking about general information or concerned about specific symptoms you're experiencing?

Please note I can provide medical information but cannot diagnose conditions or replace professional medical advice."""
        
        elif "medication" in query.lower() or "drug" in query.lower() or "medicine" in query.lower():
            clarification = f"""I notice your question about "{query}" could be made more specific:
            
1) Are you asking about specific dosages, side effects, or interactions?
2) Do you have any other medical conditions or take other medications that might be relevant?
3) Are you looking for general information or have concerns about a specific situation?

Please note I can provide general medication information but cannot give personalized medical advice."""
        
        else:
            clarification = f"""I notice your question about "{query}" could be interpreted in multiple ways:
            
1) Could you provide more context or specific details about your question?
2) Are you looking for general information or information about a specific situation?
3) Would it help if I explained some of the common terms or concepts related to this topic first?

Please note I can provide medical information but cannot diagnose conditions or replace professional medical advice."""
        
        return clarification
    
    return model_response

print("Fallback mechanism for ambiguous queries created")
```

# [CELL 6] - Implement Hallucination Detection
```python
def detect_hallucinations(response, query, expected_concepts=None):
    """
    Detect potential hallucinations in model responses
    Returns a hallucination score and flagged issues
    """
    hallucination_indicators = {
        "definitive_claims": [
            r"100% effective", r"always works", r"cures all", r"completely safe",
            r"guaranteed to", r"never causes", r"all patients", r"everyone with"
        ],
        "specific_numbers": [
            r"\d{2,3}% of (patients|people|cases)", r"studies show \d{2,3}%"
        ],
        "unverifiable_claims": [
            r"recent studies show", r"doctors agree that", r"research has proven",
            r"it is well established", r"it is widely accepted"
        ],
        "medical_advice": [
            r"you should", r"you must", r"you need to", r"I recommend",
            r"take \d+ (mg|milligrams)", r"increase your dosage", r"reduce your dosage"
        ]
    }
    
    issues = []
    hallucination_score = 0
    
    # Check for definitive claims
    for pattern in hallucination_indicators["definitive_claims"]:
        if re.search(pattern, response, re.IGNORECASE):
            issues.append(f"Definitive claim detected: '{re.search(pattern, response, re.IGNORECASE).group(0)}'")
            hallucination_score += 2
    
    # Check for specific unverifiable numbers
    for pattern in hallucination_indicators["specific_numbers"]:
        if re.search(pattern, response, re.IGNORECASE):
            issues.append(f"Specific unverifiable statistic: '{re.search(pattern, response, re.IGNORECASE).group(0)}'")
            hallucination_score += 1.5
    
    # Check for unverifiable claims
    for pattern in hallucination_indicators["unverifiable_claims"]:
        if re.search(pattern, response, re.IGNORECASE):
            issues.append(f"Unverifiable claim: '{re.search(pattern, response, re.IGNORECASE).group(0)}'")
            hallucination_score += 1
    
    # Check for medical advice
    for pattern in hallucination_indicators["medical_advice"]:
        if re.search(pattern, response, re.IGNORECASE):
            issues.append(f"Medical advice detected: '{re.search(pattern, response, re.IGNORECASE).group(0)}'")
            hallucination_score += 2
    
    # Check for expected concepts (if provided)
    if expected_concepts:
        found_concepts = 0
        for concept in expected_concepts:
            if concept.lower() in response.lower():
                found_concepts += 1
        
        concept_coverage = found_concepts / len(expected_concepts)
        if concept_coverage < 0.5:
            issues.append(f"Low coverage of expected medical concepts: {found_concepts}/{len(expected_concepts)}")
            hallucination_score += 1
    
    # Normalize score between 0-10
    normalized_score = min(10, hallucination_score)
    
    return {
        "score": normalized_score,
        "issues": issues,
        "severity": "High" if normalized_score > 7 else "Medium" if normalized_score > 4 else "Low"
    }

print("Hallucination detection system implemented")
```

# [CELL 7] - Load Test Queries
```python
# Load test queries from file
with open("evaluation/input_queries.json", "r") as f:
    test_queries = json.load(f)

print(f"Loaded {len(test_queries)} test queries")
```

# [CELL 8] - Run Tests and Collect Results
```python
def generate_response(prompt_func, query):
    """Generate a response using the specified prompt function"""
    prompt = prompt_func(query)
    result = pipe(prompt, return_full_text=False)[0]["generated_text"]
    return result

# Create results storage
results = {
    "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
    "model": model_name,
    "responses": [],
    "hallucinations": []
}

# Process each test query with each prompt type
print("Running tests on all queries with all prompt types...")
for query_data in tqdm(test_queries):
    query = query_data["query"]
    query_id = query_data["id"]
    expected_concepts = query_data.get("expected_concepts", [])
    
    query_results = {
        "query_id": query_id,
        "query": query,
        "category": query_data["category"],
        "responses": {}
    }
    
    for prompt_name, prompt_func in prompt_types.items():
        print(f"\nProcessing query {query_id} with {prompt_name} prompt...")
        
        # Generate initial response
        response = generate_response(prompt_func, query)
        
        # Check if response is ambiguous and needs clarification
        clarification = handle_ambiguous_input(query, response)
        needed_clarification = clarification != response
        
        if needed_clarification:
            print(f"Ambiguity detected, generating clarification for query {query_id}...")
            final_response = clarification
            clarification_status = "Clarification requested"
        else:
            final_response = response
            clarification_status = "Direct answer"
        
        # Check for hallucinations
        hallucination_check = detect_hallucinations(final_response, query, expected_concepts)
        
        # Store results
        query_results["responses"][prompt_name] = {
            "prompt": prompt_func(query),
            "response": final_response,
            "clarification_status": clarification_status,
            "hallucination_score": hallucination_check["score"],
            "hallucination_issues": hallucination_check["issues"],
            "hallucination_severity": hallucination_check["severity"]
        }
        
        # Track hallucinations for analysis
        if hallucination_check["score"] > 3:
            results["hallucinations"].append({
                "query_id": query_id,
                "query": query,
                "prompt_type": prompt_name,
                "response_excerpt": final_response[:200] + "...",
                "issues": hallucination_check["issues"],
                "score": hallucination_check["score"]
            })
    
    results["responses"].append(query_results)

# Save results to file
with open("evaluation/output_logs.json", "w") as f:
    json.dump(results, f, indent=2)

print(f"\nCompleted testing {len(test_queries)} queries with {len(prompt_types)} prompt types")
print(f"Results saved to evaluation/output_logs.json")
print(f"Detected {len(results['hallucinations'])} potential hallucinations")
```

# [CELL 9] - Analyze Results and Generate Report
```python
def analyze_results(results):
    """
    Analyze the results and generate evaluation metrics
    """
    # Extract data for analysis
    prompt_types = list(results["responses"][0]["responses"].keys())
    metrics = {
        "accuracy": {pt: [] for pt in prompt_types},
        "clarity": {pt: [] for pt in prompt_types},
        "hallucination": {pt: [] for pt in prompt_types},
        "ambiguity": {pt: [] for pt in prompt_types}
    }
    
    # Calculate metrics for each query and prompt type
    for query_result in results["responses"]:
        for prompt_type in prompt_types:
            response_data = query_result["responses"][prompt_type]
            
            # Hallucination score (lower is better)
            hallucination_score = 10 - response_data["hallucination_score"]  # Invert for consistency
            metrics["hallucination"][prompt_type].append(hallucination_score)
            
            # Ambiguity handling (1 for direct answer, 0 for needing clarification)
            ambiguity_score = 1 if response_data["clarification_status"] == "Direct answer" else 0
            metrics["ambiguity"][prompt_type].append(ambiguity_score)
            
            # Clarity score (based on response length and structure)
            response_text = response_data["response"]
            has_structure = bool(re.search(r"^\d[\)\.]\s", response_text, re.MULTILINE))
            word_count = len(response_text.split())
            clarity_score = min(10, (word_count / 100) * 5) if word_count < 200 else 10 - min(5, (word_count - 200) / 100)
            if has_structure:
                clarity_score += 2
            clarity_score = min(10, clarity_score)
            metrics["clarity"][prompt_type].append(clarity_score)
            
            # Accuracy is harder to automatically evaluate without ground truth
            # Here we use a proxy based on expected concepts coverage
            expected_concepts = query_result.get("expected_concepts", [])
            if expected_concepts:
                found_concepts = sum(1 for concept in expected_concepts if concept.lower() in response_text.lower())
                accuracy_score = (found_concepts / len(expected_concepts)) * 10
            else:
                # If no expected concepts, use inverse of hallucination as proxy
                accuracy_score = hallucination_score
            metrics["accuracy"][prompt_type].append(accuracy_score)
    
    # Calculate averages
    averages = {
        metric: {pt: sum(scores) / len(scores) for pt, scores in prompt_data.items()}
        for metric, prompt_data in metrics.items()
    }
    
    # Calculate overall scores
    overall_scores = {
        pt: (
            averages["accuracy"][pt] * 0.4 +  # Weight accuracy higher
            averages["hallucination"][pt] * 0.3 +
            averages["clarity"][pt] * 0.2 +
            averages["ambiguity"][pt] * 0.1
        )
        for pt in prompt_types
    }
    
    # Find best prompt type for each metric
    best_for_metric = {
        metric: max(averages[metric].items(), key=lambda x: x[1])[0]
        for metric in averages
    }
    
    # Find overall best prompt type
    best_overall = max(overall_scores.items(), key=lambda x: x[1])[0]
    
    # Generate hallucination analysis
    hallucination_by_type = {}
    for h in results["hallucinations"]:
        if h["prompt_type"] not in hallucination_by_type:
            hallucination_by_type[h["prompt_type"]] = []
        hallucination_by_type[h["prompt_type"]].append(h)
    
    hallucination_counts = {pt: len(hallucinations) for pt, hallucinations in hallucination_by_type.items()}
    
    # Generate common hallucination patterns
    all_issues = [issue for h in results["hallucinations"] for issue in h["issues"]]
    issue_counts = {}
    for issue in all_issues:
        issue_type = issue.split(":")[0].strip()
        if issue_type not in issue_counts:
            issue_counts[issue_type] = 0
        issue_counts[issue_type] += 1
    
    # Return analysis results
    return {
        "metrics": metrics,
        "averages": averages,
        "overall_scores": overall_scores,
        "best_for_metric": best_for_metric,
        "best_overall": best_overall,
        "hallucination_counts": hallucination_counts,
        "common_issues": issue_counts
    }

# Generate analysis report
print("Analyzing results...")
analysis = analyze_results(results)

# Create visualization for the analysis
plt.figure(figsize=(12, 8))

# Plot average scores by prompt type
metrics = list(analysis["averages"].keys())
prompt_types = list(analysis["averages"]["accuracy"].keys())
x = np.arange(len(metrics))
width = 0.2
multiplier = 0

for prompt_type in prompt_types:
    offset = width * multiplier
    scores = [analysis["averages"][metric][prompt_type] for metric in metrics]
    plt.bar(x + offset, scores, width, label=prompt_type)
    multiplier += 1

plt.xlabel('Metrics')
plt.ylabel('Score (0-10)')
plt.title('Performance by Prompt Type and Metric')
plt.xticks(x + width, metrics)
plt.ylim(0, 10)
plt.legend(loc='upper left', ncols=2)
plt.tight_layout()
plt.savefig("evaluation/prompt_comparison.png")

# Generate markdown report
report = f"""# Medical QA System Evaluation Report

## Overview
- **Model:** {model_name}
- **Evaluation Date:** {results["timestamp"]}
- **Queries Tested:** {len(results["responses"])}
- **Prompt Types Evaluated:** {", ".join(prompt_types)}

## Performance Summary

### Overall Scores (0-10 scale)
{pd.DataFrame({"Score": analysis["overall_scores"]}).sort_values("Score", ascending=False).to_markdown()}

### Best Performing Prompt Types
- **Overall Best:** {analysis["best_overall"]}
- **Best for Accuracy:** {analysis["best_for_metric"]["accuracy"]}
- **Best for Clarity:** {analysis["best_for_metric"]["clarity"]}
- **Best for Hallucination Avoidance:** {analysis["best_for_metric"]["hallucination"]}
- **Best for Ambiguity Handling:** {analysis["best_for_metric"]["ambiguity"]}

## Detailed Metrics
{pd.DataFrame({(metric, pt): score for metric, pt_scores in analysis["averages"].items() for pt, score in pt_scores.items()}).unstack().to_markdown()}

## Hallucination Analysis

### Hallucination Counts by Prompt Type
{pd.DataFrame({"Count": analysis["hallucination_counts"]}).to_markdown()}

### Common Hallucination Issues
{pd.DataFrame({"Count": analysis["common_issues"]}).sort_values("Count", ascending=False).to_markdown()}

## Recommendations

Based on the evaluation results, we recommend:

1. **Preferred Prompt Type:** {analysis["best_overall"]} prompts performed best overall and should be the default choice.

2. **Hallucination Mitigation:**
   - Add explicit uncertainty statements in responses
   - Include disclaimers about medical advice
   - Implement stronger detection for definitive claims
   - Avoid specific statistics unless well-established

3. **Ambiguity Handling:**
   - The fallback mechanism effectively identified ambiguous queries
   - Consider adding more domain-specific clarification templates
   - Implement a confidence threshold for triggering clarifications

4. **Future Improvements:**
   - Expand few-shot examples with more diverse medical scenarios
   - Fine-tune the model on high-quality medical information
   - Implement stronger guardrails for medical advice detection
   - Create a medical terminology database for concept verification

## Conclusion

The evaluation demonstrates that {analysis["best_overall"]} prompting is most effective for medical QA with the Qwen3 4B model. The system shows promising results in providing medical information while maintaining appropriate limitations and avoiding hallucinations. Further improvements should focus on enhancing accuracy while maintaining the system's ability to express uncertainty when appropriate.
"""

# Save the report
with open("evaluation/analysis_report.md", "w") as f:
    f.write(report)

print("Analysis complete! Report saved to evaluation/analysis_report.md")
```

# [CELL 10] - Create Utils Module
```python
# Define helper functions that can be imported in other modules
def format_response(response, query, hallucination_check):
    """Format a response with hallucination warnings if needed"""
    formatted = f"Q: {query}\n\n"
    
    if hallucination_check["score"] > 5:
        formatted += f"⚠️ WARNING: This response may contain unreliable information (Hallucination score: {hallucination_check['score']}/10)\n\n"
    
    formatted += f"A: {response}\n"
    
    if hallucination_check["issues"]:
        formatted += "\nPotential issues detected:\n"
        for issue in hallucination_check["issues"]:
            formatted += f"- {issue}\n"
    
    return formatted

def get_best_prompt_for_query_type(query, analysis_results):
    """Determine the best prompt type based on query characteristics"""
    query_lower = query.lower()
    
    if "symptoms" in query_lower or "signs" in query_lower:
        return "cot"  # Chain of thought for symptom analysis
    elif "medication" in query_lower or "drug" in query_lower or "medicine" in query_lower:
        return "few_shot"  # Few-shot for medication information
    elif "treatment" in query_lower or "therapy" in query_lower:
        return "meta"  # Meta-prompt for treatment options
    else:
        return analysis_results["best_overall"]  # Default to overall best

def save_example_to_log(query, response, prompt_type, hallucination_check):
    """Save an example to the log for future reference"""
    with open("evaluation/example_log.md", "a") as f:
        f.write(f"## Query: {query}\n")
        f.write(f"**Prompt type:** {prompt_type}\n")
        f.write(f"**Hallucination score:** {hallucination_check['score']}/10\n")
        f.write(f"**Response:**\n\n{response}\n\n")
        f.write("---\n\n")

print("Utility functions defined")
```

# [CELL 11] - Interactive Demo
```python
def run_interactive_demo(analysis_results):
    """Run an interactive demo of the medical QA system"""
    print(f"{Fore.GREEN}===== Medical QA System Demo =====\n{Style.RESET_ALL}")
    print(f"Using {model_name} with optimal prompt strategies")
    print(f"Type 'exit' to quit the demo\n")
    
    while True:
        query = input(f"{Fore.BLUE}Enter your medical question: {Style.RESET_ALL}")
        if query.lower() == 'exit':
            break
        
        # Determine best prompt type for this query
        best_prompt = get_best_prompt_for_query_type(query, analysis_results)
        prompt_func = prompt_types[best_prompt]
        
        print(f"\n{Fore.YELLOW}Processing with {best_prompt} prompt...{Style.RESET_ALL}")
        
        # Generate response
        response = generate_response(prompt_func, query)
        
        # Check for ambiguity
        clarification = handle_ambiguous_input(query, response)
        if clarification != response:
            print(f"\n{Fore.RED}Ambiguity detected. Requesting clarification:{Style.RESET_ALL}")
            print(f"{clarification}\n")
            continue
        
        # Check for hallucinations
        hallucination_check = detect_hallucinations(response, query)
        
        # Format and display response
        formatted_response = format_response(response, query, hallucination_check)
        print(f"\n{formatted_response}\n")
        
        # Save interesting examples
        if hallucination_check["score"] > 3 or len(hallucination_check["issues"]) > 0:
            save_example_to_log(query, response, best_prompt, hallucination_check)

# Run the interactive demo if this notebook is executed
if __name__ == "__main__":
    run_interactive_demo(analysis)
```

print("Medical QA System implementation complete!") 